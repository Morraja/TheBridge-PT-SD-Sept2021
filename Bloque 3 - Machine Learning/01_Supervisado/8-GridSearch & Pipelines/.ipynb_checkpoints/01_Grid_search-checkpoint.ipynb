{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch y Pipelines\n",
    "\n",
    "En este notebook vamos a ver un par de cosas que ya hemos mencionado en algún que otro notebook:\n",
    " - **GridSearch**: es una herramienta de optimización que se utiliza para buscar la mejor combinación de hiperparámetros. Consiste en definir una red de valores para cada parámetro, y el objeto se encargará de gestionar la combinación de cada uno de ellos, devolviendo el mejor resultado en cada caso.\n",
    " \n",
    " - **Pipelines**: son una interfaz para trabajar con combinaciones de objetos de ``sklearn`` como si fueran uno solo. De este modo, podremos diseñar combinaciones de tratamientos de datos con modelos y utilizarlos en nuestros GridSearch, o aplicarles cross validation, como vimos en el notebook pasado. En este caso los implementaremos directamente, en lugar de utilizar una función específica.\n",
    " \n",
    "Para implementar la búsqueda de la combinación de parámetros óptima, vamos a ver 3 estrategias, que irán avanzando de menor a mayor complejidad:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Método simple\n",
    "\n",
    "Itera un algoritmo sobre un conjunto de hiperparámetros. Para ello, utilizaremos el objeto ``GridSearchCV``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100],\n",
       "                         'coef0': [-10, -1, 0, 0.1, 0.5, 1, 10, 100],\n",
       "                         'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'rbf', 'sigmoid']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "# Cargamos los datos:\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state=42)\n",
    "\n",
    "# Creamos modelo:\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Definimos parámetros:\n",
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'coef0': [-10, -1, 0, 0.1, 0.5, 1, 10, 100]\n",
    "    \n",
    "}\n",
    "\n",
    "# Nos creamos iterador basado en cross validation:\n",
    "grid = GridSearchCV(estimator = svc,\n",
    "                   param_grid = parameters,\n",
    "                   n_jobs = -1,\n",
    "                   scoring = 'accuracy',\n",
    "                   cv = 10)\n",
    "\n",
    "# Entrenamos sobre train:\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: SVC(C=0.1, coef0=-10, kernel='linear')\n",
      "Best params: {'C': 0.1, 'coef0': -10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best score: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos los mejores parámetros:\n",
    "print(\"Best estimator:\", grid.best_estimator_)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y lo probamos sobre test:\n",
    "best_estimator = grid.best_estimator_\n",
    "best_estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí lo estaríamos haciendo ya bien por fin, ya que dividimos entre train y test. Luego hacemos el cross validation sobre train. Y, finalmente, probamos el desempeño sobre test para verificar que no nos sale nada desorbitado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Vamos a poner esto en práctica con el último ejemplo que vimos para la selección de variables. Prueba a variar los parámetros partiendo de un algoritmo de regresión logística para predecir si una persona ha sobrevivido o no.\n",
    "\n",
    "Para que no andes rebuscando en los notebooks, te dejo por aquí el tratamiento común que le habíamos dado el otro día, pero te recomiendo probar por ti mismo a cambiar cosas y crear nuevas variables también:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../../data/titanic.csv\", sep='\\t')\n",
    "df\n",
    "# El que queda de Embarked, lo vamos a rellenar con \"S\" porque es el mayoritario.\n",
    "df['Embarked'] = df['Embarked'].fillna('S')\n",
    "# En Age, vamos a completar los valores nulos con la media de los valores de Age.\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "# En Pclass, vamos a convertirlo a string para luego hacer el One Hot Encoding.\n",
    "df['Pclass'] = df['Pclass'].astype(str)\n",
    "# Separamos las variables que podemos utilizar para predecir de la que queremos predecir:\n",
    "y_col = 'Sex'\n",
    "X_cols = [col for col in df.columns if col not in ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Sex']]\n",
    "X = df[X_cols]\n",
    "y = df[y_col]\n",
    "# Tratamiento categóricas:\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forma pro\n",
    "\n",
    "\n",
    "En este caso, además, introduciremos los pipelines, creándolos utilizando directamente su constructor ``Pipeline(steps=[])``. Si te fijas bien en el código, con este método podemos comparar diferentes algoritmos, con sus diferentes combinaciones de parámetros, gracias a estos pipelines. La sintáxis es el objeto ``Pipeline``, que recibe como parámetros ``steps``, que es una lista de tuplas, donde la primera se corresponde con el nombre que le das al paso, y el segundo el obejto que se ejecuta en ese paso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('classifier',\n",
       "                                        DecisionTreeClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'classifier': [LogisticRegression(C=1.5)],\n",
       "                          'classifier__C': array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5]),\n",
       "                          'classifier__penalty': ['l1', 'l2']},\n",
       "                         {'classifier': [DecisionTreeClassifier()],\n",
       "                          'classifier__criterion': ['entropy', 'gini'],\n",
       "                          'classifier__max_depth': [10, 8, 5, 2]},\n",
       "                         {'classifier': [SVC()],\n",
       "                          'classifier__kernel': ['linear', 'rbf', 'sigmoid']}])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "logistic_params = {\n",
    "    'classifier': [LogisticRegression()],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__C': np.arange(0, 4, 0.5)\n",
    "}\n",
    "\n",
    "decision_tree_params = {\n",
    "    'classifier': [DecisionTreeClassifier()],\n",
    "    'classifier__max_depth': [10, 8, 5, 2],\n",
    "    'classifier__criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "svc_params = {\n",
    "    'classifier': [svm.SVC()],\n",
    "    'classifier__kernel': ['linear', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "search_space = [logistic_params, decision_tree_params, svc_params]\n",
    "\n",
    "grid = GridSearchCV(pipe,\n",
    "                   search_space,\n",
    "                   cv=10,\n",
    "                   n_jobs=-1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('classifier', LogisticRegression(C=1.5))])\n",
      "{'classifier': LogisticRegression(C=1.5), 'classifier__C': 1.5, 'classifier__penalty': 'l2'}\n",
      "0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(grid.predict(X_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Bueno pues, para que podamos comparar qué nos ofrece una versión frente a otra, vamos a volver sobre el mismo ejemplo del Titanic para probar estas técnicas.\n",
    "\n",
    "1. Utiliza el GridSearch para encontrar la combinación de parámetros que mejores resultados ofrece, probando, esta vez, con otros algoritmos como Árbol de decisión y SVC.\n",
    "\n",
    "Utiliza la misma separación train/test de antes. También sería conveniente que utilizaras una semilla apra tener cierta reproducibilidad de los datos, es decir, que vuelva a salir lo mismo si vuelves a ejecutar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../../data/titanic.csv\", sep='\\t')\n",
    "df\n",
    "# El que queda de Embarked, lo vamos a rellenar con \"S\" porque es el mayoritario.\n",
    "df['Embarked'] = df['Embarked'].fillna('S')\n",
    "# En Age, vamos a completar los valores nulos con la media de los valores de Age.\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "# En Pclass, vamos a convertirlo a string para luego hacer el One Hot Encoding.\n",
    "df['Pclass'] = df['Pclass'].astype(str)\n",
    "# Separamos las variables que podemos utilizar para predecir de la que queremos predecir:\n",
    "y_col = 'Sex'\n",
    "X_cols = [col for col in df.columns if col not in ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Sex']]\n",
    "X = df[X_cols]\n",
    "y = df[y_col]\n",
    "# Tratamiento categóricas:\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Next Level\n",
    "\n",
    "Finalmente, tenemos otra posible implementación, donde guardamos cada uno de los grids para tener un mayor control de cada modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos definiendo los pipelines principales:\n",
    "reg_log = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer()),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"reglog\", LogisticRegression())\n",
    "])\n",
    "\n",
    "svc = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"selectkbest\", SelectKBest()),\n",
    "    (\"svc\", svm.SVC())\n",
    "])\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Y definimos sus parámetros:\n",
    "re_log_param = {\n",
    "    \"imputer__strategy\": ['mean', 'median', 'most_frequent'],\n",
    "    \"reglog__penalty\": [\"l1\", \"l2\"],\n",
    "    \"reglog__C\": np.arange(0, 4, 0.5)\n",
    "}\n",
    "\n",
    "svc_param = {\n",
    "    \"selectkbest__k\": [1, 2, 3],\n",
    "    \"svc__C\": np.arange(0.1, 0.9, 0.1),\n",
    "    \"svc__kernel\": ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "decision_tree_params = {\n",
    "    'max_depth': [10, 100, 500, 1000],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Nos creamos los grids de cada uno:\n",
    "gs_reg_log = GridSearchCV(reg_log,\n",
    "                         re_log_param,\n",
    "                         cv = 10,\n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1,\n",
    "                         verbose=1)\n",
    "\n",
    "gs_svm = GridSearchCV(svc,\n",
    "                         svc_param,\n",
    "                         cv = 10,\n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1,\n",
    "                         verbose=1)\n",
    "\n",
    "gs_decision_tree = GridSearchCV(decision_tree,\n",
    "                         decision_tree_params,\n",
    "                         cv = 10,\n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1,\n",
    "                         verbose=1)\n",
    "grids = {\n",
    "    \"gs_reg_log\": gs_reg_log,\n",
    "    \"gs_svm\": gs_svm,\n",
    "    \"gs_rand_forest\": gs_decision_tree\n",
    "}\n",
    "#gs_reg_log.fit(X_train, y_train)\n",
    "#print(\"Best estimator:\", gs_reg_log.best_estimator_)\n",
    "#print(\"Best params:\", gs_reg_log.best_params_)\n",
    "#print(\"Best score:\", gs_reg_log.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "for nombre, grid_search in grids.items():\n",
    "    grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('gs_reg_log', GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('imputer', SimpleImputer()),\n",
       "                                       ('scaler', StandardScaler()),\n",
       "                                       ('reglog', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'imputer__strategy': ['mean', 'median',\n",
       "                                               'most_frequent'],\n",
       "                         'reglog__C': array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5]),\n",
       "                         'reglog__penalty': ['l1', 'l2']},\n",
       "             scoring='accuracy', verbose=1)), ('gs_svm', GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('selectkbest', SelectKBest()),\n",
       "                                       ('svc', SVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'selectkbest__k': [1, 2, 3],\n",
       "                         'svc__C': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]),\n",
       "                         'svc__kernel': ['linear', 'poly', 'rbf']},\n",
       "             scoring='accuracy', verbose=1)), ('gs_rand_forest', GridSearchCV(cv=10, estimator=DecisionTreeClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [10, 100, 500, 1000]},\n",
       "             scoring='accuracy', verbose=1))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs_svm</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs_reg_log</td>\n",
       "      <td>0.941667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gs_rand_forest</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Grid  Best score\n",
       "1          gs_svm    0.950000\n",
       "0      gs_reg_log    0.941667\n",
       "2  gs_rand_forest    0.925000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grids = [(i, j.best_score_) for i, j in grids.items()]\n",
    "\n",
    "best_grids = pd.DataFrame(best_grids, columns = ['Grid', 'Best score'])\n",
    "best_grids.sort_values(by='Best score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: Pipeline(steps=[('scaler', StandardScaler()), ('selectkbest', SelectKBest(k=2)),\n",
      "                ('svc', SVC(C=0.1, kernel='linear'))])\n",
      "Best params: {'selectkbest__k': 2, 'svc__C': 0.1, 'svc__kernel': 'linear'}\n",
      "Best score: 0.9499999999999998\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\", gs_svm.best_estimator_)\n",
    "print(\"Best params:\", gs_svm.best_params_)\n",
    "print(\"Best score:\", gs_svm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos mejores resultados:\n",
    "estimador = gs_svm.best_estimator_\n",
    "estimador.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podemos predecir:\n",
    "estimador.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraemos los nombres de las variables:\n",
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.72477507e-23, 2.69962606e-14, 1.93619072e-72, 3.57639330e-65])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sacamos los pvalores de cada uno\n",
    "estimador['selectkbest'].pvalues_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos guardar el modelo, podemos ayudarnos de **pickle**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Escribir\n",
    "with open('finished_model.model', 'wb') as archivo_salida:\n",
    "    pickle.dump(estimador, archivo_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer\n",
    "with open('finished_model.model', 'rb') as archivo_entrada:\n",
    "    pipeline_importado = pickle.load(archivo_entrada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez importado, ya podemos utilizarlo como si lo acabásemos de crear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('selectkbest', SelectKBest(k=2)),\n",
       "                ('svc', SVC(C=0.1, kernel='linear'))])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_importado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_flowers = np.array([[6.9, 3.1, 5.1, 2.3],\n",
    "                        [5.8, 2.7, 3.9, 1.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline_importado' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6cb952bb47a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline_importado\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpipeline_importado\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_flowers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline_importado' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline_importado.predict(X_test)\n",
    "pipeline_importado.predict(new_flowers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svm.best_estimator_.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Para terminar este ejemplo, y que se entienda de la mejor manera posible, vamos a repetir el experimento con el mismo dataset y el mismo objetivo. La única diferecia será que, esta vez, nos centraremos en añadir pasos a los pipelines principales (como selectores de variables o escalados).\n",
    "\n",
    "También nos interesará saber cuál es la mejor combinación para cada modelo. Utiliza, al menos, 3 de estos modelos: LogisticRegression, DecisionTreeClasiffier, LinearSVC, SVC y KNN; y realiza la combinación variando, al menos, 2 parámetros diferentes para cada uno.\n",
    "\n",
    "Además, finaliza guardando el modelo que mejor resultado te ofrezca para utilizarlo en cualquier otro momento.\n",
    "\n",
    "Como hemos comentado antes, utiliza la misma separación train/test de antes. También sería conveniente que utilizaras una semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../../data/titanic.csv\", sep='\\t')\n",
    "df\n",
    "# El que queda de Embarked, lo vamos a rellenar con \"S\" porque es el mayoritario.\n",
    "df['Embarked'] = df['Embarked'].fillna('S')\n",
    "# En Age, vamos a completar los valores nulos con la media de los valores de Age.\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "# En Pclass, vamos a convertirlo a string para luego hacer el One Hot Encoding.\n",
    "df['Pclass'] = df['Pclass'].astype(str)\n",
    "# Separamos las variables que podemos utilizar para predecir de la que queremos predecir:\n",
    "y_col = 'Sex'\n",
    "X_cols = [col for col in df.columns if col not in ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Sex']]\n",
    "X = df[X_cols]\n",
    "y = df[y_col]\n",
    "# Tratamiento categóricas:\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
