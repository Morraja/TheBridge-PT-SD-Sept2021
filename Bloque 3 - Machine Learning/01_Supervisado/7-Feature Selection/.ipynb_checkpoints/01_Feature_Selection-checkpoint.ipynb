{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección de variables (Feature Selection)\n",
    "\n",
    "En este notebook veremos los métodos más utilizados para seleccionar variables de nuestro dataframe que nos ayude a focalizar el problema. Pero, ¿por qué seleccionar variables, dónde queda eso de que necesitamos datos y cuantos más datos mejor? Pues sí y no, porque obtendremos mejores resultados cuanta más información **útil** dispongamos para extraer patrones.\n",
    "\n",
    "Existen dos razones principales por las que hacer selección de variables:\n",
    " - **Reducir complejidad coputacional.** Cuantos menos datos tengamos, menor carga tendrá que soportar el modelo, por lo que podrá realizar los cálculos mucho más rápido, y necesitaremos menores recursos para implementarlo.\n",
    " - **Reducir overfitting.** Algunos algoritmos se ven afectados por las variables que no aportan información para predecir la variable objetivo, ya que generan ruido y favorecen que el resultado sea menos preciso, pues el algoritmo no es capaz de detectar que no es información útil y hacen overfitting con estas variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto en la teoría, tenemos diferentes tipos de reducción de variables. Podemos diferenciar 3 tipos diferentes:\n",
    "\n",
    " - **Supervisado**: buscamos extraer la información comparando con la varaible objetivo.\n",
    " - **No supervisado**: como no tenemos variable objetivo, solo podemos buscar relaciones entre las propias variables\n",
    " - **Reducción de dimensionalidad**: buscamos reducir variables creándonos otras que sean combinaciones de las originales. Lo que buscamos aquí consiste en sacar la información mínima necesaria de las variables que tenemos para crearnos un conjunto de menos variables que las originales. Dentro de estos métodos entra nuestro amigo PCA, sobre el cual volveremos mucho más adelante.\n",
    " \n",
    "En este notebook veremos los 2 primeros, que son los que más vamos a utilizar, y que no suponen la pérdida de las variables originales. Para facilitar el seguimiento del notebook, se ha decidido dividirlo con el siguiente formato:\n",
    " - Selección basada en estadísticos (no supervisado y supervisado)\n",
    " - Selección basada en modelo (Solo supervisado):\n",
    "   - Intrínseco\n",
    "   - Wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección basada en estadísticos\n",
    "\n",
    "La primera opción que se nos viene a la cabeza a la hora de reducir variables, es la eliminación de aquellas varaibles que no presentan cambios, es decir, las constantes, ya que no aportan absolutamente nada de información. Partiendo de esta premisa, es razonable asumir que las características con baja varianza son peores que aquellas con alta varianza. Por lo tanto, se puede considerar que las características con variación por debajo de un cierto umbral no serán útiles para nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basado en estadísticos: No supervisado\n",
    "\n",
    "Para ello, nos vamos a basar en el objeto de ``sklearn`` llamado ``VarianceThreshold``, y es que para casi todo lo que te puedas imaginar hay un objeto de ``sklearn``. Su tilización será exactamente igual que los que hemos visto hasta la fecha: ``fit`` con los datos y posterior ``transform`` (o ``fit_transform`` directamente). Cuando lo creamos, podemos indicar el umbral que queremos utilizar para mantener o no una variable en nuestro dataset.\n",
    "\n",
    "Además, utilizaremos la función ``make_classification``, que nos generará automáticamente un dataset aleatorio con un target binario.\n",
    "\n",
    "Para comprobar el funcionamiento del objeto, utilizaremos el atributo ``shape``, que nos indicará cuántas filas (las mismas siempre) y columnas tiene la salida, pudiendo comprobar que se reducen las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:17.058096Z",
     "start_time": "2020-11-19T10:10:13.843481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "x_data_generated, y_data_generated = make_classification()\n",
    "x_data_generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:17.825065Z",
     "start_time": "2020-11-19T10:10:17.818065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empezamos con un umbral relativamente bajo, 0.7, lo que significa que se eliminarán aquellas variables que tengan una varainza de 0.7 o inferior:\n",
    "umbral = 0.7\n",
    "\n",
    "# selector_varianza = VarianceThreshold(umbral)\n",
    "# selector_varianza.fit(x_data_generated)\n",
    "# x_transformada = selector_varianza.transform(x_data_generated)\n",
    "\n",
    "VarianceThreshold(umbral).fit_transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:18.837626Z",
     "start_time": "2020-11-19T10:10:18.833589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subiendo el umbral de varianza, nos vamos quedando con las variables que varían más, es decir, con aquellas que más información nos aportan:\n",
    "umbral = 0.8\n",
    "\n",
    "VarianceThreshold(umbral).fit_transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:20.212419Z",
     "start_time": "2020-11-19T10:10:20.207432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umbral = 0.9\n",
    "\n",
    "VarianceThreshold(umbral).fit_transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Basado en estadísticos: Supervisado (filtrado)\n",
    "\n",
    "Además de lo que acabamos de ver para el caso \"No supervisado\", cuando disponemos de un objetivo podemos aplicar otras técnicas. En este caso, lo que vamos a hacer es utilizar la función ``f_classif``, que nos devuelve la relación de cada variable con el objetivo. En el caso de clasificación utilizamos ``f_classif``, mientras que para los modelos de regresión utilizaremos ``f_regression``.\n",
    "\n",
    "Si te fijas en el código, utilizaremos este objeto como argumento de otro objeto: ``SelectKBest``, que es un objeto que, dada una función de scoring (como es ``f_classif``) y un valor entero ``k``, me devolverá únicamente las ``k`` varaibles con mayor score en base a esa funciónd e scoring. En el ejemplo, nos quedaremos con las 5 variables que mayor relación tienen con el objetivo.\n",
    "\n",
    "Por si te interesa, existen otras funciones de scoring, como son ``mutual_info_classif`` o ``mutual_info_regression``.\n",
    "\n",
    "Además, utilizaremos la función ``cross_val_score`` para validar los resultados. Lo que hace esta función es evaluar un modelo haciendo *cross validation* sobre un conjunto de datos, buscando minimizar el error (el valor negativo es porque internamente busca maximizar algo, y queremos que el error sea mínimo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:22.132032Z",
     "start_time": "2020-11-19T10:10:22.121062Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "x_data_kbest = SelectKBest(f_classif, k=5).fit_transform(x_data_generated, y_data_generated)\n",
    "x_data_varth = VarianceThreshold(.9).fit_transform(x_data_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_kbest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a comprobar si realmente mejora nuestro modelo o no. Para ello, utilizaremos el mimsmo modelo en todos, que en este caso será una regresión logística:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:24.915747Z",
     "start_time": "2020-11-19T10:10:24.911758Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(solver='lbfgs', random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrica \"neg_log_loss\" cuanto más cercade 0, mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:25.414124Z",
     "start_time": "2020-11-19T10:10:25.386181Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8400000000000001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(logit, x_data_generated, y_data_generated, \n",
    "                scoring='accuracy', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:25.923503Z",
     "start_time": "2020-11-19T10:10:25.907511Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8700000000000001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logit, x_data_kbest, y_data_generated, \n",
    "                scoring='accuracy', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:26.516993Z",
     "start_time": "2020-11-19T10:10:26.500038Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logit, x_data_varth, y_data_generated, \n",
    "                scoring='accuracy', cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes observar, el valor se aproxima más a 0 cuando utilizamos nuestra selección de variables, lo que nos hace suponer que esto realmente funciona, auqnue para comprobarlo deberíamos utilizar un caso real, y no siempre nos va a resultar de utilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Prueba la selección de variables que acabamos de ver con el conjunto de datos del Titanic. En este caso, intenta predecir si se trata de un hombre o de una mujer. Para ello, créate 1 modelo (el que quieras) y ejecútalo con las diferentes combinaciones, haciendo uso de la función ``cross_val_score``, por lo que no será necesario que dividas en train y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pears, Mrs. Thomas (Edith Wearne)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113776</td>\n",
       "      <td>66.6000</td>\n",
       "      <td>C2</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Meo, Mr. Alfonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>55.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 11206</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>van Billiard, Mr. Austin Blyler</td>\n",
       "      <td>male</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>A/5. 851</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Olsen, Mr. Ole Martin</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fa 265302</td>\n",
       "      <td>7.3125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Williams, Mr. Charles Duane</td>\n",
       "      <td>male</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC 17597</td>\n",
       "      <td>61.3792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "151          152         1       1   \n",
       "152          153         0       3   \n",
       "153          154         0       3   \n",
       "154          155         0       3   \n",
       "155          156         0       1   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "151                  Pears, Mrs. Thomas (Edith Wearne)  female  22.0      1   \n",
       "152                                   Meo, Mr. Alfonzo    male  55.5      0   \n",
       "153                    van Billiard, Mr. Austin Blyler    male  40.5      0   \n",
       "154                              Olsen, Mr. Ole Martin    male   NaN      0   \n",
       "155                        Williams, Mr. Charles Duane    male  51.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "151      0            113776  66.6000    C2        S  \n",
       "152      0        A.5. 11206   8.0500   NaN        S  \n",
       "153      2          A/5. 851  14.5000   NaN        S  \n",
       "154      0         Fa 265302   7.3125   NaN        S  \n",
       "155      1          PC 17597  61.3792   NaN        C  \n",
       "\n",
       "[156 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../../data/titanic.csv\", sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156 entries, 0 to 155\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  156 non-null    int64  \n",
      " 1   Survived     156 non-null    int64  \n",
      " 2   Pclass       156 non-null    int64  \n",
      " 3   Name         156 non-null    object \n",
      " 4   Sex          156 non-null    object \n",
      " 5   Age          126 non-null    float64\n",
      " 6   SibSp        156 non-null    int64  \n",
      " 7   Parch        156 non-null    int64  \n",
      " 8   Ticket       156 non-null    object \n",
      " 9   Fare         156 non-null    float64\n",
      " 10  Cabin        31 non-null     object \n",
      " 11  Embarked     155 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 14.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El que queda de Embarked, lo vamos a rellenar con \"S\" porque es el mayoritario.\n",
    "df['Embarked'] = df['Embarked'].fillna('S')\n",
    "\n",
    "# En Age, vamos a completar los valores nulos con la media de los valores de Age.\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "\n",
    "# En Pclass, vamos a convertirlo a string para luego hacer el One Hot Encoding.\n",
    "df['Pclass'] = df['Pclass'].astype(str)\n",
    "\n",
    "# Separamos las variables que podemos utilizar para predecir de la que queremos predecir:\n",
    "y_col = 'Sex'\n",
    "X_cols = [col for col in df.columns if col not in ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Sex']]\n",
    "\n",
    "X = df[X_cols]\n",
    "y = df[y_col]\n",
    "\n",
    "# Tratamiento categóricas:\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Selección de k mejores variables respecto target:\n",
    "k_param = 5\n",
    "selector = SelectKBest(f_classif, k=k_param)\n",
    "selector.fit(X, y)\n",
    "X_data_kbest = selector.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_kbest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección en base a la varainza:\n",
    "umbral = 0.7\n",
    "var_thresh = VarianceThreshold(umbral)\n",
    "var_thresh.fit(X)\n",
    "X_data_varth = var_thresh.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_varth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.687925</td>\n",
       "      <td>Survived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.428586</td>\n",
       "      <td>SibSp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.943985</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.985267</td>\n",
       "      <td>Embarked_Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.680493</td>\n",
       "      <td>Parch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score    variable\n",
       "0  77.687925    Survived\n",
       "2   5.428586       SibSp\n",
       "1   4.943985         Age\n",
       "9   1.985267  Embarked_Q\n",
       "3   1.680493       Parch"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'score': selector.scores_, 'variable': X.columns}).sort_values(by=\"score\", ascending=False).iloc[:k_param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1542.490926</td>\n",
       "      <td>Fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171.126191</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.108481</td>\n",
       "      <td>SibSp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.752301</td>\n",
       "      <td>Parch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           var variables\n",
       "4  1542.490926      Fare\n",
       "1   171.126191       Age\n",
       "2     1.108481     SibSp\n",
       "3     0.752301     Parch"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'var': var_thresh.variances_, 'variables': X.columns}).sort_values(by=\"var\", ascending=False).iloc[:X_data_varth.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:24.915747Z",
     "start_time": "2020-11-19T10:10:24.911758Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(solver='lbfgs', random_state=17, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:25.414124Z",
     "start_time": "2020-11-19T10:10:25.386181Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.776008064516129"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(logit, X, y, scoring='accuracy', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:25.923503Z",
     "start_time": "2020-11-19T10:10:25.907511Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7762096774193548"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logit, X_data_kbest, y, scoring='accuracy', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:26.516993Z",
     "start_time": "2020-11-19T10:10:26.500038Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6350806451612903"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logit, X_data_varth, y, scoring='accuracy', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7887096774193548"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe = make_pipeline(SelectKBest(f_classif, k=k_param), StandardScaler(), logit)\n",
    "\n",
    "cross_val_score(pipe, X, y, scoring='accuracy', cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección basada en modelo\n",
    "\n",
    "Otro enfoque que se suele dar a este probleam consiste en utilizar algún modelo de referencia para la selección de variables, basando en la imprtancia de estas en la toma de decisiones del modelo. Se suelen utilizar dos tipos de modelos: alguno basado en árboles como los árboles de decisión o un modelo lineal con regularización Lasso, para que sea propenso a eliminar aquellos pesos de las variables menos importantes.\n",
    "\n",
    "La premisa es clara: si las varaibles no aportan nada de información a un modelo simple, no es necesario arrastrarlas a uno más complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisado intrínseco\n",
    "\n",
    "Para esta implementación, nos vamos a ayudar del objeto ``SelectFromModel``, que recibirá como un modelo como parámetro ``estimator`` y nos devolverá las variables más relevantes para dicho modelo.\n",
    "\n",
    "Además, utilizaremos un objeto que nos servirá de enlace con el próximo notebook: ``make_pipeline``. Este objeto nos permitirá enlazar más de un objeto de ``sklearn`` para pasárselo a nuestro ``cross_val_score`` y que podemos evaluarlo de forma directa, sin tener que hacer cosas \"por fuera\". Como veremos en el próximo notebook, estos \"pipelines\" nos serán de mucha utilidad.\n",
    "\n",
    "Finalmente, comprobamos cómo funcionan diferentes algoritmos: *LogisticRegression*, *DecisionTreeClassifier* y *DecisionTreeClassifier* con selección de variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:29.773202Z",
     "start_time": "2020-11-19T10:10:29.554472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4627710184957575\n",
      "-6.907819246776553\n",
      "-0.3817654235989549\n"
     ]
    }
   ],
   "source": [
    "# Synthetic example\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x_data_generated, y_data_generated = make_classification()\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=10, random_state=17)\n",
    "\n",
    "pipe = make_pipeline(SelectFromModel(estimator=clf), logit)\n",
    "\n",
    "print(cross_val_score(logit, x_data_generated, y_data_generated, scoring='neg_log_loss', cv=5).mean())\n",
    "print(cross_val_score(clf, x_data_generated, y_data_generated, scoring='neg_log_loss', cv=5).mean())\n",
    "print(cross_val_score(pipe, x_data_generated, y_data_generated, scoring='neg_log_loss', cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes observar, en este caso nos mejora el modelo de regresión logística basándose en las mejores varaibles seleccionadas por el árbol de decisión, que por sí solo no es precisamente bueno. Sin embargo, esto no siempre es así, dependerá mucho de los datos, como todo en este mundo, auqnue por lo general suele mejorar más veces de las que empeora los resultados. Además, el número de variables modificará enormemente este resultado.\n",
    "\n",
    "Por otra parte, podemos aplicar la estandarización a nuestros datos para ver si mejoran los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:12:40.084628Z",
     "start_time": "2020-11-19T10:12:39.986914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR + selection:  -0.3818523933512572\n",
      "LR:  -0.4598550072248944\n",
      "DT:  -6.907819246776553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_data = x_data_generated\n",
    "y_data = y_data_generated\n",
    "\n",
    "pipe1 = make_pipeline(StandardScaler(), SelectFromModel(estimator=clf), logit)\n",
    "\n",
    "pipe2 = make_pipeline(StandardScaler(), logit)\n",
    "\n",
    "print('LR + selection: ', cross_val_score(pipe1, x_data_generated, y_data_generated, scoring='neg_log_loss', cv=5).mean())\n",
    "print('LR: ', cross_val_score(pipe2, x_data_generated, y_data_generated, scoring='neg_log_loss', cv=5).mean())\n",
    "print('DT: ', cross_val_score(clf, x_data_generated, y_data_generated, scoring='neg_log_loss', cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, y contra lo que podíamos imaginar, los resultados empeoran, poniendo de manifiesto que no siempre podemos anticiparnos a lo que va a pasar, que necesitamos probar y probar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Utiliza el DataFrame que te dejo a continuación para trabajar los conceptos que acabamos de ver. Prueba los 2 modelos de clasificación que te apetezcan y compáralos entre sí y con sus versiones basadas en selección de variables con estimador basado en Árbol de decisión y Regresión Logística, así como con y sin estandarización. Es decir, deberías terminar con un total de 10 modelos:\n",
    " - Modelo A\n",
    " - Modelo B\n",
    " - Modelo A con selección de variables basado en RL\n",
    " - Modelo A con selección de variables basado en DT\n",
    " - Modelo B con selección de variables basado en RL\n",
    " - Modelo B con selección de variables basado en DT\n",
    " - Modelo A con selección de variables basado en RL y escalado\n",
    " - Modelo A con selección de variables basado en DT y escalado\n",
    " - Modelo B con selección de variables basado en RL y escalado\n",
    " - Modelo B con selección de variables basado en DT y escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../../data/vgsales.csv\")\n",
    "\n",
    "def get_platform(x):\n",
    "    if x in ['DS', 'Wii', 'GBA', 'GC', '3DS', 'N64']:\n",
    "        return 'Nintendo'\n",
    "    elif x in ['PS2', 'PS3', 'PS', 'PSP', 'PS4', 'PSV', 'SNES']:\n",
    "        return 'Sony'\n",
    "    elif x in ['X360', 'PC', 'XB', 'XOne']:\n",
    "        return 'Microsoft'\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "df['Platform'] = df['Platform'].apply(lambda x: get_platform(x))\n",
    "\n",
    "df = df[df['Platform'] != '']\n",
    "df = df[df['Publisher'].isin(df['Publisher'].value_counts()[:20].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>genre__Action</th>\n",
       "      <th>genre__Adventure</th>\n",
       "      <th>genre__Fighting</th>\n",
       "      <th>genre__Misc</th>\n",
       "      <th>...</th>\n",
       "      <th>Publi__Nintendo</th>\n",
       "      <th>Publi__Sega</th>\n",
       "      <th>Publi__Sony Computer Entertainment</th>\n",
       "      <th>Publi__Square Enix</th>\n",
       "      <th>Publi__THQ</th>\n",
       "      <th>Publi__Take-Two Interactive</th>\n",
       "      <th>Publi__Tecmo Koei</th>\n",
       "      <th>Publi__Ubisoft</th>\n",
       "      <th>Publi__Unknown</th>\n",
       "      <th>Publi__Warner Bros. Interactive Entertainment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.49</td>\n",
       "      <td>29.02</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.46</td>\n",
       "      <td>82.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.85</td>\n",
       "      <td>12.88</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.31</td>\n",
       "      <td>35.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.75</td>\n",
       "      <td>11.01</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.96</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.38</td>\n",
       "      <td>9.23</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.90</td>\n",
       "      <td>30.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nintendo</td>\n",
       "      <td>14.03</td>\n",
       "      <td>9.20</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.85</td>\n",
       "      <td>29.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Platform  NA_Sales  EU_Sales  JP_Sales  Other_Sales  Global_Sales  \\\n",
       "0  Nintendo     41.49     29.02      3.77         8.46         82.74   \n",
       "2  Nintendo     15.85     12.88      3.79         3.31         35.82   \n",
       "3  Nintendo     15.75     11.01      3.28         2.96         33.00   \n",
       "6  Nintendo     11.38      9.23      6.50         2.90         30.01   \n",
       "7  Nintendo     14.03      9.20      2.93         2.85         29.02   \n",
       "\n",
       "   genre__Action  genre__Adventure  genre__Fighting  genre__Misc  ...  \\\n",
       "0              0                 0                0            0  ...   \n",
       "2              0                 0                0            0  ...   \n",
       "3              0                 0                0            0  ...   \n",
       "6              0                 0                0            0  ...   \n",
       "7              0                 0                0            1  ...   \n",
       "\n",
       "   Publi__Nintendo  Publi__Sega  Publi__Sony Computer Entertainment  \\\n",
       "0                1            0                                   0   \n",
       "2                1            0                                   0   \n",
       "3                1            0                                   0   \n",
       "6                1            0                                   0   \n",
       "7                1            0                                   0   \n",
       "\n",
       "   Publi__Square Enix  Publi__THQ  Publi__Take-Two Interactive  \\\n",
       "0                   0           0                            0   \n",
       "2                   0           0                            0   \n",
       "3                   0           0                            0   \n",
       "6                   0           0                            0   \n",
       "7                   0           0                            0   \n",
       "\n",
       "   Publi__Tecmo Koei  Publi__Ubisoft  Publi__Unknown  \\\n",
       "0                  0               0               0   \n",
       "2                  0               0               0   \n",
       "3                  0               0               0   \n",
       "6                  0               0               0   \n",
       "7                  0               0               0   \n",
       "\n",
       "   Publi__Warner Bros. Interactive Entertainment  \n",
       "0                                              0  \n",
       "2                                              0  \n",
       "3                                              0  \n",
       "6                                              0  \n",
       "7                                              0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_dummy = pd.get_dummies(df['Genre'], prefix='genre_')\n",
    "publisher_dummy = pd.get_dummies(df['Publisher'], prefix='Publi_')\n",
    "\n",
    "df = df.join(genre_dummy)\n",
    "df = df.join(publisher_dummy)\n",
    "\n",
    "df = df.drop(['Rank', 'Name', 'Genre', 'Publisher', 'Year'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Platform', axis=1)\n",
    "y = df['Platform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "logit = LogisticRegression(random_state=42, max_iter=2500)\n",
    "clf = DecisionTreeClassifier(max_depth= 5, random_state=42)\n",
    "model_knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN + selection LR:  0.3814310046168453\n",
      "KNN + slection DT:  0.3995145427003834\n"
     ]
    }
   ],
   "source": [
    "pipe1 = make_pipeline(SelectFromModel(estimator=logit), model_knn)\n",
    "pipe2 = make_pipeline(SelectFromModel(estimator=clf), model_knn)\n",
    "\n",
    "print('KNN + selection LR: ', cross_val_score(pipe1, X, y, scoring='accuracy', cv=5).mean())\n",
    "print('KNN + slection DT: ', cross_val_score(pipe2, X, y, scoring='accuracy', cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN + selection LR:  0.44287693969109904\n",
      "KNN + slection DT:  0.423899941776048\n"
     ]
    }
   ],
   "source": [
    "pipe3 = make_pipeline(SelectFromModel(estimator=logit), StandardScaler(), model_knn)\n",
    "pipe4 = make_pipeline(SelectFromModel(estimator=clf), StandardScaler(), model_knn)\n",
    "\n",
    "print('KNN + selection LR: ', cross_val_score(pipe3, X, y, scoring='accuracy', cv=5).mean())\n",
    "print('KNN + slection DT: ', cross_val_score(pipe4, X, y, scoring='accuracy', cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisado wrapper\n",
    "\n",
    "Por último, vamos a utilizar otro método que suele emplearse muy a menudo: la selección de variables de forma recursiva. Para ello, nos basaremos en el objeto de ``sklearn`` llamado ``RFE``, que recibe como parámetros el modelo a utilizar para evaluar la importancia de las variables (``estimator``), el número de variables que seleccionar (``n_features_to_select``) y el número de variables que eliminar en cada iteración (``step``).\n",
    "\n",
    "En este caso, vamos a probar el ejemplo sobre el dataset de dígitos, y obtendremos las variables más importantes, que en este caso serán los píxeles. No hace falta que entiendas la mayoría del código, solamente quédate con la parte donde se utiliza el RFE, que es el objeto que nos permitirá reducir las variables de forma recursiva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:36:46.595134Z",
     "start_time": "2020-11-19T10:36:36.919545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargamos los datos en \"X\" e \"y\":\n",
    "digits = load_digits()\n",
    "X = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target\n",
    "\n",
    "# Creamos el objeto RFE y lo utilizamos para rankear la importancia de cada pixel:\n",
    "svc = SVC(kernel=\"linear\", C=1)\n",
    "rfe = RFE(estimator=svc, n_features_to_select=1, step=1)\n",
    "rfe.fit(X, y)\n",
    "ranking = rfe.ranking_.reshape(digits.images[0].shape)\n",
    "\n",
    "# Graficamos este ranking:\n",
    "plt.matshow(ranking, cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "plt.title(\"Pixels más importantes según RFE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:42:05.336475Z",
     "start_time": "2020-11-19T10:41:59.409990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(kernel=\"linear\", C=1)\n",
    "rfe = RFE(estimator=svc, n_features_to_select=10, step=1)\n",
    "rfe.fit_transform(X, y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Vuelve a leer el DataFrame del Titanic y crea un modelo que sea capaz de predecir si una persona ha sobrevivido o no. Para ello, realiza lo siguiente:\n",
    " 1. Modelo regresión logística\n",
    " 2. Modelo regresión logística con estandarización\n",
    " 3. Modelo regresión logística con selección de varaibles basado en varianza\n",
    " 4. Modelo regresión logística con selección de variables basado en árbol de decisión\n",
    " 5. Modelo regresión logística con selección de variables basado en árbol de decisión y escalado\n",
    " 6. Modelo regresión logística con selección de variables basado en RFE\n",
    " 7. Modelo regresión logística con selección de variables basado en RFE y escalado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../../../data/titanic.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tranamiento del Dataset\n",
    "df.drop(['PassengerId', 'Cabin', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "df['Age'].fillna(np.mean(df['Age']), inplace=True)\n",
    "df['Embarked'].fillna('S', inplace=True)\n",
    "\n",
    "pclass_dummy = pd.get_dummies(df['Pclass'], prefix='pclass_')\n",
    "gender_dummy = pd.get_dummies(df['Sex'], prefix='gender_')\n",
    "embarked_dummy = pd.get_dummies(df['Embarked'], prefix='embarked_')\n",
    "\n",
    "df = df.join(pclass_dummy)\n",
    "df = df.join(gender_dummy)\n",
    "df = df.join(embarked_dummy)\n",
    "\n",
    "df.drop(['Pclass', 'Sex', 'Embarked'], axis=1, inplace=True)\n",
    "\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=2500, random_state=42)\n",
    "log_reg.fit(X, y)\n",
    "log_reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6538461538461539"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 Scaled X\n",
    "scaler = MinMaxScaler()\n",
    "scaled_x = scaler.fit_transform(X)\n",
    "\n",
    "log_reg.fit(scaled_x, y)\n",
    "log_reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:  0.794758064516129\n",
      "LR + MinMaxScaler:  0.8076612903225808\n"
     ]
    }
   ],
   "source": [
    "pipe1 = make_pipeline(log_reg)\n",
    "pipe2 = make_pipeline(scaler, log_reg)\n",
    "\n",
    "print('LR: ', cross_val_score(pipe1, X, y, scoring='accuracy', cv=5).mean())\n",
    "print('LR + MinMaxScaler: ', cross_val_score(pipe2, X, y, scoring='accuracy', cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "var_thresh = VarianceThreshold(0.7)\n",
    "svc = SVC(kernel=\"linear\", C=1)\n",
    "rfe = RFE(estimator=svc, n_features_to_select=4, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = make_pipeline(SelectFromModel(estimator=var_thresh), log_reg)\n",
    "pipe4 = make_pipeline(SelectFromModel(estimator=dt), log_reg)\n",
    "pipe5 = make_pipeline(MinMaxScaler(), SelectFromModel(estimator=dt), log_reg)\n",
    "pipe6 = make_pipeline(SelectFromModel(estimator=rfe), log_reg)\n",
    "pipe7 = make_pipeline(MinMaxScaler(), SelectFromModel(estimator=rfe), log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator VarianceThreshold has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator VarianceThreshold has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator VarianceThreshold has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator VarianceThreshold has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator VarianceThreshold has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR + Selection: Var_Thresh nan\n",
      "LR + Selection: DT 0.7887096774193548\n",
      "LR + Selection: DT + MinMaxScaler  0.8012096774193548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator RFE has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator RFE has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator RFE has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR + Selection: RFE nan\n",
      "LR + Selection: DT + MinMaxScaler nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator RFE has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator RFE has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator RFE has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator RFE has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator RFE has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator RFE has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 77, in transform\n",
      "    mask = self.get_support()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\", line 46, in get_support\n",
      "    mask = self._get_support_mask()\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 180, in _get_support_mask\n",
      "    scores = _get_feature_importances(estimator, self.norm_order)\n",
      "  File \"D:\\Users\\costa\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 30, in _get_feature_importances\n",
      "    raise ValueError(\n",
      "ValueError: The underlying estimator RFE has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "print('LR + Selection: Var_Thresh', cross_val_score(pipe3, X, y, scoring='accuracy', cv=5).mean())\n",
    "print('LR + Selection: DT', cross_val_score(pipe4, X, y, scoring='accuracy', cv=5).mean())\n",
    "print('LR + Selection: DT + MinMaxScaler ', cross_val_score(pipe5, X, y, scoring='accuracy', cv=5).mean())\n",
    "print('LR + Selection: RFE', cross_val_score(pipe6, X, y, scoring='accuracy', cv=5).mean())\n",
    "print('LR + Selection: DT + MinMaxScaler', cross_val_score(pipe7, X, y, scoring='accuracy', cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
